{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FI5kDIkmyBmV",
    "outputId": "dac2ede2-9fce-4fe8-e1ec-df5817e1ddd2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pwEHBMLgpVyj"
   },
   "source": [
    "##Reading files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "MuA1PAROyCaa"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "dataset = pd.read_excel(\"PERC_mendelly.xlsx\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IZv-VsdIphom"
   },
   "source": [
    "##Exploring data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JV9MjUyk3Wj1",
    "outputId": "cc0dd2f1-7e12-4b65-b2b8-7f10f9b60194"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of                                                   Poem Emotion\n",
       "0    A Tree\\nA tree beside the sandy river-beach \\n...     sad\n",
       "1    Sri Krishna\\n\\nO immense Light and thou, O spi...    love\n",
       "2    Who\\n\\n\\nIn the blue of the sky, in the green ...   peace\n",
       "3    Revelation\\n\\n\\nSomeone leaping from the rocks...     sad\n",
       "4    The Silver Call\\n\\n\\nThere is a godhead of unr...     joy\n",
       "..                                                 ...     ...\n",
       "711  Daughter Taken By Mothers Lies\\n\\nHave you any...     sad\n",
       "712  Involuntary Acceptance\\n\\nEven though\\nWe’re f...     sad\n",
       "713  Victim Of Poverty\\n\\nPoverty stricken youth ju...     sad\n",
       "714  Rain\\n\\nI sit and watch\\nas the rain falls \\nf...     sad\n",
       "715  The Face Of Sadness\\n\\nIts happened again.\\n\\n...     sad\n",
       "\n",
       "[716 rows x 2 columns]>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df = pd.DataFrame(dataset, columns=[\"Poem\", \"Emotion\"])\n",
    "\n",
    "df = df.dropna()\n",
    "df.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0AP_EliIhE3u",
    "outputId": "135de3ea-3805-4490-85bc-af438b45b087"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sad' 'love' 'peace' 'joy' 'courage' 'surprise' 'hate' 'anger' 'fear']\n"
     ]
    }
   ],
   "source": [
    "print(df.Emotion.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Inrmahz-pozg"
   },
   "source": [
    "##Converting the labels to integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "W9_-m6fE3bmd"
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(df['Emotion'])\n",
    "df['Emotion']=le.transform(df['Emotion'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "lRTx3TQ255U0",
    "outputId": "8fb09488-f1f7-48bc-d239-84eebcffa56a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Poem</th>\n",
       "      <th>Emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A Tree\\nA tree beside the sandy river-beach \\n...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sri Krishna\\n\\nO immense Light and thou, O spi...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Who\\n\\n\\nIn the blue of the sky, in the green ...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Revelation\\n\\n\\nSomeone leaping from the rocks...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Silver Call\\n\\n\\nThere is a godhead of unr...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>711</th>\n",
       "      <td>Daughter Taken By Mothers Lies\\n\\nHave you any...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>712</th>\n",
       "      <td>Involuntary Acceptance\\n\\nEven though\\nWe’re f...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>713</th>\n",
       "      <td>Victim Of Poverty\\n\\nPoverty stricken youth ju...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>714</th>\n",
       "      <td>Rain\\n\\nI sit and watch\\nas the rain falls \\nf...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>715</th>\n",
       "      <td>The Face Of Sadness\\n\\nIts happened again.\\n\\n...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>716 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Poem  Emotion\n",
       "0    A Tree\\nA tree beside the sandy river-beach \\n...        7\n",
       "1    Sri Krishna\\n\\nO immense Light and thou, O spi...        5\n",
       "2    Who\\n\\n\\nIn the blue of the sky, in the green ...        6\n",
       "3    Revelation\\n\\n\\nSomeone leaping from the rocks...        7\n",
       "4    The Silver Call\\n\\n\\nThere is a godhead of unr...        4\n",
       "..                                                 ...      ...\n",
       "711  Daughter Taken By Mothers Lies\\n\\nHave you any...        7\n",
       "712  Involuntary Acceptance\\n\\nEven though\\nWe’re f...        7\n",
       "713  Victim Of Poverty\\n\\nPoverty stricken youth ju...        7\n",
       "714  Rain\\n\\nI sit and watch\\nas the rain falls \\nf...        7\n",
       "715  The Face Of Sadness\\n\\nIts happened again.\\n\\n...        7\n",
       "\n",
       "[716 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "8NtoLqjX6MR1"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fvyjeC8YptC2"
   },
   "source": [
    "##Loading the pre-trained BERT model for fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tZIymUwR6-h1",
    "outputId": "e862a64c-6285-4ef3-f332-ae82aec86276"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModelForSequenceClassification\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n\u001b[1;32m      4\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbert-base-uncased\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers'"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch.nn.functional as F\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1hHKxZET7HW8"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ocOSqxYQ8TKJ"
   },
   "outputs": [],
   "source": [
    "# Define a custom dataset class for annotated poems\n",
    "class PoemDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.texts = data['Poem'].tolist()\n",
    "        self.labels = data['Emotion'].tolist()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer(text, truncation=True, padding='max_length', max_length=128, return_tensors='pt')\n",
    "        return {'input_ids': encoding['input_ids'][0], 'attention_mask': encoding['attention_mask'][0], 'label': label}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XTgqJ4q58X9T"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, val_data = train_test_split(df, test_size=0.2, random_state=72)\n",
    "\n",
    "\n",
    "train_dataset = PoemDataset(train_data, tokenizer)\n",
    "val_dataset = PoemDataset(val_data, tokenizer)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "wgfoSLwW8iB5",
    "outputId": "69d38e6d-4098-4fa7-e7d1-c85d274dd1b9"
   },
   "outputs": [],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "pfeb-Mxn8m9g",
    "outputId": "732bc0ee-acdf-4832-cd74-fedbbb28b48e"
   },
   "outputs": [],
   "source": [
    "val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9g-HLhN58qRc"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    f1 = f1_score(labels, preds, average=\"weighted\")\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\"accuracy\": acc, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ftNltRpjp1oh"
   },
   "source": [
    "##Defining and Training the model and fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "dYuxNIPQNCOz",
    "outputId": "ffecc29a-47b3-4728-de09-6a40312cb61c"
   },
   "outputs": [],
   "source": [
    "!pip install accelerate -U\n",
    "!pip install transformers[torch]\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "batch_size = 16\n",
    "logging_steps = len(train_data) // batch_size\n",
    "training_args = TrainingArguments(output_dir=\"results\",\n",
    "                                  num_train_epochs=10,\n",
    "                                  learning_rate=0.0001,\n",
    "                                  per_device_train_batch_size=batch_size,\n",
    "                                  per_device_eval_batch_size=batch_size,\n",
    "                                  load_best_model_at_end=True,\n",
    "                                  metric_for_best_model=\"f1\",\n",
    "                                  weight_decay=0.01,\n",
    "                                  evaluation_strategy=\"epoch\",\n",
    "                                  save_strategy=\"epoch\",\n",
    "                                  disable_tqdm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ou4FYDX0NC5e"
   },
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m18_9GORNajJ"
   },
   "outputs": [],
   "source": [
    "results = trainer.evaluate()\n",
    "results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "conQdCSSRurW"
   },
   "outputs": [],
   "source": [
    "model.save_pretrained('./model')\n",
    "tokenizer.save_pretrained('./model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W56MuXFJp_yL"
   },
   "source": [
    "##Testing the model on a random poem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z9qV0gf3RViT"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Load the tokenizer and the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/content/model\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"/content/model\")\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Read text from a file\n",
    "with open('/content/LustPoemsiMInLustPoembyEffieYalenaSteyn.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Tokenize the text and truncate or pad it to the desired length\n",
    "inputs = tokenizer(text, padding=True, truncation=True, max_length=512, return_tensors='pt')\n",
    "\n",
    "# Convert tokenized input to tensors\n",
    "input_ids = inputs['input_ids']\n",
    "attention_mask = inputs['attention_mask']\n",
    "\n",
    "# Pass the input tensors through the model and get the predicted output probabilities\n",
    "outputs = model(input_ids, attention_mask=attention_mask)\n",
    "probs = torch.softmax(outputs.logits, dim=-1)\n",
    "\n",
    "print(outputs)\n",
    "print(probs)\n",
    "# Extract the highest probability class index\n",
    "preds = torch.argmax(probs, dim=-1)\n",
    "print(preds)\n",
    "print(preds.item())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sl7wDyQ8qDS9"
   },
   "source": [
    "##Testing another sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "laEwJSxecSo-"
   },
   "outputs": [],
   "source": [
    "# Read text from a file\n",
    "with open('/content/AlonePoemsIAmMuchTooAloneInThisWorldYetNotAlonePoembyRainerMariaRilke.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Tokenize the text and truncate or pad it to the desired length\n",
    "inputs1 = tokenizer(text, padding=True, truncation=True, max_length=512, return_tensors='pt')\n",
    "\n",
    "# Convert tokenized input to tensors\n",
    "input_ids1 = inputs1['input_ids']\n",
    "attention_mask1 = inputs1['attention_mask']\n",
    "\n",
    "# Pass the input tensors through the model and get the predicted output probabilities\n",
    "outputs1 = model(input_ids1, attention_mask=attention_mask1)\n",
    "probs1 = torch.softmax(outputs1.logits, dim=-1)\n",
    "\n",
    "print(outputs1)\n",
    "print(probs1)\n",
    "# Extract the highest probability class index\n",
    "preds1 = torch.argmax(probs1, dim=-1)\n",
    "print(preds1)\n",
    "print(preds1.item())"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
